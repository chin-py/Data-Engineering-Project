{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import  col, round\n",
    "from pyspark.sql.functions import to_timestamp, year, month, dayofmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2333952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datbricks creats session automatically we can see using \n",
    "# spark\n",
    "# Manually creating\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeatherDataProcessing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY = \"AKIAUYXOGVO7D65JFF4A\"\n",
    "SECRET_KEY = \"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\"\n",
    "BUCKET_NAME = \"openweather-etl-extracted-data\"\n",
    "FILE_PATH = \"s3a://\" + BUCKET_NAME + \"/*.csv\"\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"Asia/Kolkata\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", ACCESS_KEY)\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", SECRET_KEY)\n",
    "# spark.conf.set(\"fs.s3a.endpoint\", \"s3.ap-south-1.amazonaws.com\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "\n",
    "spark.conf.set(\"fs.s3a.access.key\", ACCESS_KEY) \n",
    "spark.conf.set(\"fs.s3a.secret.key\", SECRET_KEY)\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.ap-south-1.amazonaws.com\")\n",
    "spark.conf.set(\"com.amazonaws.services.s3.enableV4\", \"true\") \n",
    "df_raw = spark.read.option(\"header\", True).csv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_raw.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb749f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.select(\n",
    "    col(\"City\"),\n",
    "    col(\"Country\"),\n",
    "    col(\"Weather_main\"),\n",
    "    col(\"Weather_subtype\"),\n",
    "    col(\"Temperature\").cast(\"double\"),\n",
    "    col(\"Feels_Like\").cast(\"double\"),\n",
    "    col(\"Min_Temp\").cast(\"double\"),\n",
    "    col(\"Max_Temp\").cast(\"double\"),\n",
    "    col(\"Pressure\").cast(\"int\"),\n",
    "    col(\"Humidity\").cast(\"int\"),\n",
    "    col(\"Visibility\").cast(\"int\"),\n",
    "    col(\"Wind_speed\").cast(\"double\"),\n",
    "    col(\"cloudiness_percent\").cast(\"int\"),\n",
    "    col(\"Rain_mm_hour\").cast(\"double\"),\n",
    "    col(\"Snow_mm_hour\").cast(\"double\"),\n",
    "    col(\"Time\").cast(\"long\"),\n",
    "    col(\"Timezone_offset\").cast(\"int\"),\n",
    "    to_timestamp(\"Time_Recorded_local\").alias(\"Time_Recorded_local\"),\n",
    "    to_timestamp(\"Sunrise_local\").alias(\"Sunrise_local\"),\n",
    "    to_timestamp(\"Sunset_local\").alias(\"Sunset_local\")\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce991d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, regexp_replace\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Time_Recorded_local_clean\",\n",
    "    to_timestamp(\n",
    "        regexp_replace(\"Time_Recorded_local\", r\"\\+\\d{2}:\\d{2}$\", \"\"),  # Remove +05:30\n",
    "        \"yyyy-MM-dd'T'HH:mm:ss\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06fceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.select(expr(\"typeof(Time_Recorded_local)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "df.select(date_format(\"Time_Recorded_local\", \"yyyy-MM-dd HH:mm:ss\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8f082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Convert Kelvin to Celsius (new columns)\n",
    "df = df.withColumn(\"Temp_C\", round(col(\"Temperature\") - 273.15, 2)) \\\n",
    "       .withColumn(\"Feels_Like_C\", round(col(\"Feels_Like\") - 273.15, 2)) \\\n",
    "       .withColumn(\"Min_Temp_C\", round(col(\"Min_Temp\") - 273.15, 2)) \\\n",
    "       .withColumn(\"Max_Temp_C\", round(col(\"Max_Temp\") - 273.15, 2))\n",
    "\n",
    "# ✅ Drop rows with null Snow data (if necessary)\n",
    "df = df.dropna(subset=[\"Snow_mm_hour\"])\n",
    "\n",
    "# ✅ Derive additional features\n",
    "df = df.withColumn(\"Year\", year(\"Time_Recorded_local\")) \\\n",
    "       .withColumn(\"Month\", month(\"Time_Recorded_local\")) \\\n",
    "       .withColumn(\"Day\", dayofmonth(\"Time_Recorded_local\"))\n",
    "\n",
    "# ✅ Show basic statistics\n",
    "print(\"=== Sample Rows ===\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "print(\"=== Schema ===\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"=== Weather Condition Counts ===\")\n",
    "df.groupBy(\"Weather_main\").count().show()\n",
    "\n",
    "print(\"=== Average Temperature by Day ===\")\n",
    "df.groupBy(\"Day\").avg(\"Temp_C\").orderBy(\"Day\").show()\n",
    "\n",
    "# ✅ Save final dataframe back to S3 as a single Parquet file (or CSV if you prefer)\n",
    "df.coalesce(1).write.mode(\"overwrite\").parquet(\"s3a://your-public-bucket-name/final_output/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
